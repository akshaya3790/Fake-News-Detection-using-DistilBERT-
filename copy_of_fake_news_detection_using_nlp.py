# -*- coding: utf-8 -*-
"""Copy of Fake News Detection Using NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eISeRj0R5OPADlL0XCNArL4ACOVWMRQp
"""

# STEP 1: INSTALL DEPENDENCIES (NO WANDB!)
!pip install transformers datasets accelerate -q
!pip install scikit-learn pandas numpy -q
import os
os.environ["WANDB_DISABLED"] = "true"  # ‚úÖ Disable W&B tracking completely

# STEP 2: IMPORT LIBRARIES
import pandas as pd
import numpy as np
import re
import string
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
from datasets import Dataset

# STEP 3: UPLOAD DATA FILES
from google.colab import files
uploaded = files.upload()  # Upload Fake.csv and True.csv

fake = pd.read_csv("Fake (4).csv")[['title']]
true = pd.read_csv("True (4).csv")[['title']]
fake['label'] = 0
true['label'] = 1
data = pd.concat([fake, true], axis=0).sample(frac=1).reset_index(drop=True)

def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    return text


data['title'] = data['title'].apply(clean_text)

# OPTIONAL: limit to 5000 samples for faster training
data = data[:5000]

# STEP 5: SPLIT DATA
train_texts, test_texts, train_labels, test_labels = train_test_split(
    data['title'].tolist(), data['label'].tolist(), test_size=0.2, random_state=42)

train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})
test_df = pd.DataFrame({'text': test_texts, 'label': test_labels})

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# STEP 6: TOKENIZATION
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
def tokenize_function(example):
    return tokenizer(example["text"], truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# STEP 7: LOAD MODEL
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# STEP 8: TRAINING CONFIG
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,  # ‚úÖ Reduced for speed
    weight_decay=0.01,
    logging_dir='./logs',
    logging_strategy="no",
    save_strategy="no"
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
    compute_metrics=lambda p: {
        "accuracy": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))
    }
)

# STEP 9: TRAIN MODEL (faster)
trainer.train()

# STEP 10: EVALUATE
eval_result = trainer.evaluate()
print(f"‚úÖ Evaluation Accuracy: {eval_result['eval_accuracy']*100:.2f}%")

# STEP 11: PREDICT FUNCTION
def predict_user_input(text):
    text = clean_text(text)
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs, dim=1).item()
        confidence = round(probs[0][pred].item() * 100, 2)
        return ("Real" if pred == 1 else "Fake", confidence)

# STEP 12: USER INTERACTION
while True:
    user_input = input("üì∞ Enter a news headline (or type 'exit' to quit):\n")
    if user_input.lower() == "exit":
        break
    label, score = predict_user_input(user_input)
    print(f"üîç Prediction: {label} (Confidence: {score}%)\n")